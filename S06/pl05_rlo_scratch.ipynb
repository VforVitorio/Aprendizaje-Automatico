{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PL05. Logistic Regression _Scratch_ (with Gradient Descent)\n",
    "\n",
    "__Borja GonzÃ¡lez Seoane. Machine Learning. Course 2024-25__\n",
    "\n",
    "\n",
    "\n",
    "In this *notebook* we proceed to implement a logistic regression model from scratch (_scratch_), that is, without using Scikit-Learn libraries. The gradient descent algorithm will be implemented to minimize the cost function.\n",
    "\n",
    "In this *notebook* a _dummy_ dataset will simply be used to test the model. The part of the exercise related to working with the Titanic dataset will be performed in the next *notebook* of PL05, to also contrast between different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import (  # Interesting for generating test data\n",
    "    make_classification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of _dummy_ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.15017405,  0.78458206, -0.16330173, ...,  0.04469772,\n",
       "          0.8137789 ,  1.03922619],\n",
       "        [-0.93768773,  0.48820379,  1.64637422, ..., -0.81955796,\n",
       "         -0.57678974, -0.69439466],\n",
       "        [ 0.03517578, -0.36665391,  0.04038387, ..., -1.24407626,\n",
       "         -0.41888296,  1.0266307 ],\n",
       "        ...,\n",
       "        [ 1.00983071,  0.43162932, -0.17442641, ...,  1.6352267 ,\n",
       "         -0.90310221,  1.23752688],\n",
       "        [ 0.71331689, -1.24903904,  1.15858324, ...,  0.32303203,\n",
       "          0.75805069, -0.94071444],\n",
       "        [-1.15180736, -0.68909235, -1.52811751, ..., -0.72661141,\n",
       "         -1.2500449 , -0.63578947]]),\n",
       " array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 0, 0]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = make_classification(\n",
    "    n_samples = 1000,\n",
    "    n_features = 20,\n",
    ")\n",
    "\n",
    "display(data)\n",
    "x = data[0]\n",
    "y = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation by parts\n",
    "\n",
    "First we proceed to implement the necessary functions for the logistic regression model. The following functions will be implemented:\n",
    "\n",
    "1. `predict`: $W^TX + b$, where $W$ is the weight vector and $b$ is the bias.\n",
    "2. `sigmoid`: $\\frac{1}{1 + e^{-\\text{predict}(X)}}$.\n",
    "3. `loss`: $-\\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\text{sigmoid}(X_i)) + (1 - y_i) \\log(1 - \\text{sigmoid}(X_i))$.\n",
    "4. `dl_dw`, derivative of the cost function with respect to weights: $\\frac{1}{m} \\sum_{i=1}^{m} (\\text{sigmoid}(X_i) - y_i)X_i$.\n",
    "5. `dl_db`, derivative of the cost function with respect to bias: $\\frac{1}{m} \\sum_{i=1}^{m} (\\text{sigmoid}(X_i) - y_i)$.\n",
    "6. `update`, weight and bias update: $W = W - \\alpha \\text{dl\\_dw}$ and $b = b - \\alpha \\text{dl\\_db}$, where $\\alpha$ is the learning rate.\n",
    "7. `fit`, function that, based on the previous pieces, trains the model for a given number of iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Prediction: linear combination of weights and features.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param w: Weights.\n",
    "    :param b: Bias.\n",
    "    :return: Linear predictions.\n",
    "    \"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(y_hat):\n",
    "    \"\"\"\n",
    "    Sigmoid function to map values to probabilities in the range `[0, 1]`.\n",
    "\n",
    "    :param y_hat: Linear prediction.\n",
    "    :return: Values transformed to probabilities.\n",
    "    \"\"\"\n",
    "    # Limit `y_hat` values to avoid overflow in the exponential\n",
    "    y_hat = np.clip(y_hat, -500, 500)\n",
    "\n",
    "    return 1 / (1 + np.exp(-y_hat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss(y, sigmoid):\n",
    "    \"\"\"\n",
    "    Logistic loss function.\n",
    "\n",
    "    :param y: True labels.\n",
    "    :param sigmoid: Probabilistic predictions.\n",
    "    :return: Average loss (scalar).\n",
    "    \"\"\"\n",
    "    # Limit `sigmoid` values to avoid calculation errors\n",
    "    sigmoid = np.clip(sigmoid, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    return -(y * np.log(sigmoid) + (1 - y) * np.log(1 - sigmoid)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dldw(X, y, sigmoid):\n",
    "    \"\"\"\n",
    "    Gradient of loss with respect to weights.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param y: True labels.\n",
    "    :param sigmoid: Probabilistic predictions.\n",
    "    :return: Weight gradients.\n",
    "    \"\"\"\n",
    "    return np.dot(X.T, (sigmoid - y)) / X.shape[0]\n",
    "\n",
    "def dldb(y, sigmoid):\n",
    "    \"\"\"\n",
    "    Gradient of loss with respect to bias.\n",
    "\n",
    "    :param y: True labels.\n",
    "    :param sigmoid: Probabilistic predictions.\n",
    "    :return: Bias gradient.\n",
    "    \"\"\"\n",
    "    return (sigmoid - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(a, g, lr):\n",
    "    \"\"\"\n",
    "    Update weights or bias value using gradient descent.\n",
    "\n",
    "    :param a: Current value.\n",
    "    :param g: Gradient.\n",
    "    :param lr: Learning rate.\n",
    "    :return: Updated value.\n",
    "    \"\"\"\n",
    "    return a - (g * lr)\n",
    "\n",
    "\n",
    "def fit(X, y, learning_rate=0.1, n_iter=100, monitor_loss=False):\n",
    "    \"\"\"\n",
    "    Fit the logistic regression model.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param y: Class labels.\n",
    "    :param learning_rate: Learning rate.\n",
    "    :param n_iter: Number of iterations.\n",
    "    :param monitor_loss: Whether to monitor loss.\n",
    "    :return: Fitted weights and bias.\n",
    "    \"\"\"\n",
    "    # Initialize weights based on the number of features\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "\n",
    "    # Iterate the specified number of epochs\n",
    "    for i in range(n_iter):\n",
    "        # Initial prediction: linear combination of weights and features\n",
    "        y_hat = predict(X, w, b)\n",
    "\n",
    "        # Apply sigmoid function to get probabilities: map to `[0, 1]`\n",
    "        sig = sigmoid(y_hat)\n",
    "\n",
    "        # Calculate loss, in this case only for monitoring, if applicable\n",
    "        if monitor_loss:\n",
    "            loss_value = loss(y, sig)\n",
    "\n",
    "            # Print loss value every 10 iterations\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"[Iter. {i+1}/{n_iter}] Loss = {loss_value}\")\n",
    "\n",
    "        # Calculate gradients of weights and bias\n",
    "        grad_w = dldw(X, y, sig)\n",
    "        grad_b = dldb(y, sig)\n",
    "\n",
    "        # Update weights and bias by performing a gradient descent step\n",
    "        w = update(w, grad_w, learning_rate)\n",
    "        b = update(b, grad_b, learning_rate)\n",
    "\n",
    "    return w, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete model archetype\n",
    "\n",
    "Once the previous functions have been implemented and some simple numerical tests have been performed, we will proceed to implement the complete logistic regression model in the form of a class, with the `fit` and `predict` methods, thus following the usual archetype that has been used throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifierScratch:\n",
    "    def __init__(self, learning_rate=0.1, n_iter=100, monitor_loss=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.b = 0\n",
    "        self.w = None  # Will be reinitialized in the `fit` method\n",
    "\n",
    "        self._monitor_loss = monitor_loss\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fit the logistic regression model.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :param y: Class labels.\n",
    "        \"\"\"\n",
    "        # Initialize weights based on the number of features. Lazy initialization because\n",
    "        # we need to know the number of features in the input data, which we don't know in the\n",
    "        # constructor\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iterate the specified number of epochs\n",
    "        for i in range(self.n_iter):\n",
    "            # Initial prediction: linear combination of weights and features\n",
    "            y_hat = self.__predict(X, self.w, self.b)\n",
    "\n",
    "            # Apply sigmoid function to get probabilities: map to `[0, 1]`\n",
    "            sig = self.__sigmoid(y_hat)\n",
    "\n",
    "            # Calculate loss, in this case only for monitoring, if applicable\n",
    "            if self._monitor_loss:\n",
    "                loss = self.__loss(y, sig)\n",
    "\n",
    "                # Print loss value every 10 iterations\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"[Iter. {i+1}/{self.n_iter}] Loss = {loss}\")\n",
    "\n",
    "            # Calculate gradients of weights and bias\n",
    "            grad_w = self.__dldw(X, y, sig)\n",
    "            grad_b = self.__dldb(y, sig)\n",
    "\n",
    "            # Update weights and bias by performing a gradient descent step\n",
    "            self.w = self.__update(self.w, grad_w, self.learning_rate)\n",
    "            self.b = self.__update(self.b, grad_b, self.learning_rate)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels. Return predicted labels after applying\n",
    "        a threshold of 0.5 to probabilities.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :return: Predicted class labels.\n",
    "        \"\"\"\n",
    "        # Initial prediction: linear combination of weights and features\n",
    "        y_hat = self.__predict(X, self.w, self.b)\n",
    "\n",
    "        # Apply sigmoid function to get probabilities: map to `[0, 1]`\n",
    "        probs = self.__sigmoid(y_hat)\n",
    "\n",
    "        # Threshold probabilities to get class labels, which would be 0 or 1\n",
    "        return (probs >= 0.5).astype(int)\n",
    "\n",
    "    ###############################################################\n",
    "    # Include the functions developed in the previous cells as private methods\n",
    "    # of the class. Static methods are used for convenience, since there's no need to access\n",
    "    # class attributes, which are passed as arguments to the methods\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict(X, w, b):\n",
    "        \"\"\"\n",
    "        Prediction: linear combination of weights and features.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :param w: Weights.\n",
    "        :param b: Bias.\n",
    "        :return: Linear predictions.\n",
    "        \"\"\"\n",
    "        return np.dot(X, w) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def __sigmoid(y_hat):\n",
    "        \"\"\"\n",
    "        Sigmoid function to map values to probabilities in the range `[0, 1]`.\n",
    "\n",
    "        :param y_hat: Linear prediction.\n",
    "        :return: Values transformed to probabilities.\n",
    "        \"\"\"\n",
    "        # Limit `y_hat` values to avoid overflow in the exponential\n",
    "        y_hat = np.clip(y_hat, -500, 500)\n",
    "\n",
    "        return 1 / (1 + np.exp(-y_hat))\n",
    "\n",
    "    @staticmethod\n",
    "    def __loss(y, sigmoid):\n",
    "        \"\"\"\n",
    "        Logistic loss function.\n",
    "\n",
    "        :param y: True labels.\n",
    "        :param sigmoid: Probabilistic predictions.\n",
    "        :return: Average loss (scalar).\n",
    "        \"\"\"\n",
    "        # Limit `sigmoid` values to avoid calculation errors\n",
    "        sigmoid = np.clip(sigmoid, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        return -(y * np.log(sigmoid) + (1 - y) * np.log(1 - sigmoid)).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def __dldw(X, y, sigmoid):\n",
    "        \"\"\"\n",
    "        Gradient of loss with respect to weights.\n",
    "\n",
    "        :param X: Input data.\n",
    "        :param y: True labels.\n",
    "        :param sigmoid: Probabilistic predictions.\n",
    "        :return: Weight gradients.\n",
    "        \"\"\"\n",
    "        return np.dot(X.T, (sigmoid - y)) / X.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def __dldb(y, sigmoid):\n",
    "        \"\"\"\n",
    "        Gradient of loss with respect to bias.\n",
    "\n",
    "        :param y: True labels.\n",
    "        :param sigmoid: Probabilistic predictions.\n",
    "        :return: Bias gradient.\n",
    "        \"\"\"\n",
    "        return (sigmoid - y).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def __update(a, g, lr):\n",
    "        \"\"\"\n",
    "        Update weights or bias value using gradient descent.\n",
    "\n",
    "        :param a: Current value.\n",
    "        :param g: Gradient.\n",
    "        :param lr: Learning rate.\n",
    "        :return: Updated value.\n",
    "        \"\"\"\n",
    "        return a - (g * lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
