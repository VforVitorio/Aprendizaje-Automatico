{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PL05. Regresión Logística _Scratch_ (con Gradiente Descendente)\n",
    "\n",
    "__Borja González Seoane. Aprendizaje Automático. Curso 2024-25__\n",
    "\n",
    "\n",
    "\n",
    "En este *notebook* se procede a implementar un modelo de regresión logística desde cero (_scratch_), es decir, sin utilizar librerías Scikit-Learn. Se implementará el algoritmo de gradiente descendente para minimizar la función de coste.\n",
    "\n",
    "En este *notebook* se utilizará simplemente un conjunto de datos _dummy_ para probar el modelo. La parte del ejercicio relativa a trabajar con el conjunto de datos Titanic se realizará en el siguiente *notebook* de la PL05, para contrastar también entre diferentes modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import (  # Interesante para generar datos de prueba\n",
    "    make_classification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de datos _dummy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.15017405,  0.78458206, -0.16330173, ...,  0.04469772,\n",
       "          0.8137789 ,  1.03922619],\n",
       "        [-0.93768773,  0.48820379,  1.64637422, ..., -0.81955796,\n",
       "         -0.57678974, -0.69439466],\n",
       "        [ 0.03517578, -0.36665391,  0.04038387, ..., -1.24407626,\n",
       "         -0.41888296,  1.0266307 ],\n",
       "        ...,\n",
       "        [ 1.00983071,  0.43162932, -0.17442641, ...,  1.6352267 ,\n",
       "         -0.90310221,  1.23752688],\n",
       "        [ 0.71331689, -1.24903904,  1.15858324, ...,  0.32303203,\n",
       "          0.75805069, -0.94071444],\n",
       "        [-1.15180736, -0.68909235, -1.52811751, ..., -0.72661141,\n",
       "         -1.2500449 , -0.63578947]]),\n",
       " array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 0, 0]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = make_classification(\n",
    "    n_samples = 1000,\n",
    "    n_features = 20,\n",
    ")\n",
    "\n",
    "display(data)\n",
    "x = data[0]\n",
    "y = data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del modelo por partes\n",
    "\n",
    "Primero se procede a implementar las funciones necesarias para el modelo de regresión logística. Se implementarán las siguientes funciones:\n",
    "\n",
    "1. `predict`: $W^TX + b$, donde $W$ es el vector de pesos y $b$ es el sesgo o _bias_.\n",
    "2. `sigmoid`: $\\frac{1}{1 + e^{-\\text{predict}(X)}}$.\n",
    "3. `loss`: $-\\frac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\text{sigmoid}(X_i)) + (1 - y_i) \\log(1 - \\text{sigmoid}(X_i))$.\n",
    "4. `dl_dw`, derivada de la función de coste respecto a los pesos: $\\frac{1}{m} \\sum_{i=1}^{m} (\\text{sigmoid}(X_i) - y_i)X_i$.\n",
    "5. `dl_db`, derivada de la función de coste respecto al sesgo: $\\frac{1}{m} \\sum_{i=1}^{m} (\\text{sigmoid}(X_i) - y_i)$.\n",
    "6. `update`, actualización de los pesos y el sesgo: $W = W - \\alpha \\text{dl\\_dw}$ y $b = b - \\alpha \\text{dl\\_db}$, siendo $\\alpha$ la tasa de aprendizaje.\n",
    "7. `fit`, función que, a partir de las piezas anteriores, entrena el modelo en un número de iteraciones dado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Predicción: combinación lineal de los pesos y las características.\n",
    "\n",
    "    :param X: Datos de entrada.\n",
    "    :param w: Pesos.\n",
    "    :param b: Sesgo.\n",
    "    :return: Predicciones lineales.\n",
    "    \"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(y_hat):\n",
    "    \"\"\"\n",
    "    Función sigmoide para mapear valores a probabilidades en el rango `[0, 1]`.\n",
    "\n",
    "    :param y_hat: Predicción lineal.\n",
    "    :return: Valores transformados a probabilidades.\n",
    "    \"\"\"\n",
    "    # Se limitan los valores de `y_hat` para evitar desbordamiento en la exponencial\n",
    "    y_hat = np.clip(y_hat, -500, 500)\n",
    "\n",
    "    return 1 / (1 + np.exp(-y_hat))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss(y, sigmoid):\n",
    "    \"\"\"\n",
    "    Función de pérdida logística.\n",
    "\n",
    "    :param y: Etiquetas verdaderas.\n",
    "    :param sigmoid: Predicciones probabilísticas.\n",
    "    :return: Pérdida promedio (escalar).\n",
    "    \"\"\"\n",
    "    # Se limitan los valores de `sigmoid` para evitar errores en el cálculo\n",
    "    sigmoid = np.clip(sigmoid, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    return -(y * np.log(sigmoid) + (1 - y) * np.log(1 - sigmoid)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dldw(X, y, sigmoid):\n",
    "    \"\"\"\n",
    "    Gradiente de la pérdida con respecto a los pesos.\n",
    "\n",
    "    :param X: Datos de entrada.\n",
    "    :param y: Etiquetas verdaderas.\n",
    "    :param sigmoid: Predicciones probabilísticas.\n",
    "    :return: Gradiente de los pesos.\n",
    "    \"\"\"\n",
    "    return np.dot(X.T, (sigmoid - y)) / X.shape[0]\n",
    "\n",
    "def dldb(y, sigmoid):\n",
    "    \"\"\"\n",
    "    Gradiente de la pérdida con respecto al sesgo.\n",
    "\n",
    "    :param y: Etiquetas verdaderas.\n",
    "    :param sigmoid: Predicciones probabilísticas.\n",
    "    :return: Gradiente del sesgo.\n",
    "    \"\"\"\n",
    "    return (sigmoid - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(a, g, lr):\n",
    "    \"\"\"\n",
    "    Actualiza el valor de los pesos o el sesgo usando gradiente descendente.\n",
    "\n",
    "    :param a: Valor actual.\n",
    "    :param g: Gradiente.\n",
    "    :param lr: Tasa de aprendizaje.\n",
    "    :return: Valor actualizado.\n",
    "    \"\"\"\n",
    "    return a - (g * lr)\n",
    "\n",
    "\n",
    "def fit(X, y, learning_rate=0.1, n_iter=100, monitorizar_loss=False):\n",
    "    \"\"\"\n",
    "    Ajuste del modelo de regresión logística.\n",
    "\n",
    "    :param X: Datos de entrada.\n",
    "    :param y: Etiquetas de clase.\n",
    "    :param learning_rate: Tasa de aprendizaje.\n",
    "    :param n_iter: Número de iteraciones.\n",
    "    :param monitorizar_loss: Si se debe monitorizar la pérdida.\n",
    "    :return: Pesos y sesgo ajustados.\n",
    "    \"\"\"\n",
    "    # Inicializar los pesos en función del número de características\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "\n",
    "    # Iterar el número de épocas especificado\n",
    "    for i in range(n_iter):\n",
    "        # Predicción inicial: combinación lineal de los pesos y las características\n",
    "        y_hat = predict(X, w, b)\n",
    "\n",
    "        # Aplica la función sigmoide para obtener probabilidades: mapear a `[0, 1]`\n",
    "        sig = sigmoid(y_hat)\n",
    "\n",
    "        # Se calcula la pérdida, en este caso sólo para monitorización, si aplica\n",
    "        if monitorizar_loss:\n",
    "            loss_value = loss(y, sig)\n",
    "\n",
    "            # Imprimir el valor de la pérdida cada 10 iteraciones\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"[Iter. {i+1}/{n_iter}] Loss = {loss_value}\")\n",
    "\n",
    "        # Se calculan los gradientes de los pesos y el sesgo\n",
    "        grad_w = dldw(X, y, sig)\n",
    "        grad_b = dldb(y, sig)\n",
    "\n",
    "        # Se actualizan los pesos y el sesgo realizando un paso de gradiente descendente\n",
    "        w = update(w, grad_w, learning_rate)\n",
    "        b = update(b, grad_b, learning_rate)\n",
    "\n",
    "    return w, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquetipado del modelo completo\n",
    "\n",
    "Una vez implementadas las funciones anteriores y realizadas algunas pruebas sencillas de corte numérico, se procederá a implementar el modelo completo de regresión logística en forma de clase, con los métodos `fit` y `predict`, siguiendo así el arquetipo habitual que se viene utilizando a lo largo del curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RegresionLogisticaClassifierScratch:\n",
    "    def __init__(self, learning_rate=0.1, n_iter=100, monitorizar_loss=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.b = 0\n",
    "        self.w = None  # Se reinicializará en el método `fit`\n",
    "\n",
    "        self._monitorizar_loss = monitorizar_loss\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Ajuste del modelo de regresión logística.\n",
    "\n",
    "        :param X: Datos de entrada.\n",
    "        :param y: Etiquetas de clase.\n",
    "        \"\"\"\n",
    "        # Inicializar los pesos en función del número de características. Se inicializa tardíamente porque\n",
    "        # necesitamos saber el número de características de los datos de entrada, cosa que no sabemos en el\n",
    "        # constructor\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iterar el número de épocas especificado\n",
    "        for i in range(self.n_iter):\n",
    "            # Predicción inicial: combinación lineal de los pesos y las características\n",
    "            y_hat = self.__predict(X, self.w, self.b)\n",
    "\n",
    "            # Aplica la función sigmoide para obtener probabilidades: mapear a `[0, 1]`\n",
    "            sig = self.__sigmoid(y_hat)\n",
    "\n",
    "            # Se calcula la pérdida, en este caso sólo para monitorización, si aplica\n",
    "            if self._monitorizar_loss:\n",
    "                loss = self.__loss(y, sig)\n",
    "\n",
    "                # Imprimir el valor de la pérdida cada 10 iteraciones\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"[Iter. {i+1}/{self.n_iter}] Loss = {loss}\")\n",
    "\n",
    "            # Se calculan los gradientes de los pesos y el sesgo\n",
    "            grad_w = self.__dldw(X, y, sig)\n",
    "            grad_b = self.__dldb(y, sig)\n",
    "\n",
    "            # Se actualizan los pesos y el sesgo realizando un paso de gradiente descendente\n",
    "            self.w = self.__update(self.w, grad_w, self.learning_rate)\n",
    "            self.b = self.__update(self.b, grad_b, self.learning_rate)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicción de las etiquetas de clase. Se devuelven las etiquetas predichas tras aplicar\n",
    "        un umbral de 0.5 a las probabilidades.\n",
    "\n",
    "        :param X: Datos de entrada.\n",
    "        :return: Etiquetas de clase predichas.\n",
    "        \"\"\"\n",
    "        # Predicción inicial: combinación lineal de los pesos y las características\n",
    "        y_hat = self.__predict(X, self.w, self.b)\n",
    "\n",
    "        # Aplica la función sigmoide para obtener probabilidades: mapear a `[0, 1]`\n",
    "        probs = self.__sigmoid(y_hat)\n",
    "\n",
    "        # Umbraliza las probabilidades para obtener las etiquetas de clase, que serían 0 o 1\n",
    "        return (probs >= 0.5).astype(int)\n",
    "\n",
    "    ###############################################################\n",
    "    # Se incluyen las funciones desarrolladas en las celdas anteriores como métodos privados\n",
    "    # de la clase. Se usan métodos estáticos por conveniencia, ya que no se necesita acceder\n",
    "    # a los atributos de la clase, que se pasan como argumentos a los métodos\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict(X, w, b):\n",
    "        \"\"\"\n",
    "        Predicción: combinación lineal de los pesos y las características.\n",
    "\n",
    "        :param X: Datos de entrada.\n",
    "        :param w: Pesos.\n",
    "        :param b: Sesgo.\n",
    "        :return: Predicciones lineales.\n",
    "        \"\"\"\n",
    "        return np.dot(X, w) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def __sigmoid(y_hat):\n",
    "        \"\"\"\n",
    "        Función sigmoide para mapear valores a probabilidades en el rango `[0, 1]`.\n",
    "\n",
    "        :param y_hat: Predicción lineal.\n",
    "        :return: Valores transformados a probabilidades.\n",
    "        \"\"\"\n",
    "        # Se limitan los valores de `y_hat` para evitar desbordamiento en la exponencial\n",
    "        y_hat = np.clip(y_hat, -500, 500)\n",
    "\n",
    "        return 1 / (1 + np.exp(-y_hat))\n",
    "\n",
    "    @staticmethod\n",
    "    def __loss(y, sigmoid):\n",
    "        \"\"\"\n",
    "        Función de pérdida logística.\n",
    "\n",
    "        :param y: Etiquetas verdaderas.\n",
    "        :param sigmoid: Predicciones probabilísticas.\n",
    "        :return: Pérdida promedio (escalar).\n",
    "        \"\"\"\n",
    "        # Se limitan los valores de `sigmoid` para evitar errores en el cálculo\n",
    "        sigmoid = np.clip(sigmoid, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        return -(y * np.log(sigmoid) + (1 - y) * np.log(1 - sigmoid)).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def __dldw(X, y, sigmoid):\n",
    "        \"\"\"\n",
    "        Gradiente de la pérdida con respecto a los pesos.\n",
    "\n",
    "        :param X: Datos de entrada.\n",
    "        :param y: Etiquetas verdaderas.\n",
    "        :param sigmoid: Predicciones probabilísticas.\n",
    "        :return: Gradiente de los pesos.\n",
    "        \"\"\"\n",
    "        return np.dot(X.T, (sigmoid - y)) / X.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def __dldb(y, sigmoid):\n",
    "        \"\"\"\n",
    "        Gradiente de la pérdida con respecto al sesgo.\n",
    "\n",
    "        :param y: Etiquetas verdaderas.\n",
    "        :param sigmoid: Predicciones probabilísticas.\n",
    "        :return: Gradiente del sesgo.\n",
    "        \"\"\"\n",
    "        return (sigmoid - y).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def __update(a, g, lr):\n",
    "        \"\"\"\n",
    "        Actualiza el valor de los pesos o el sesgo usando gradiente descendente.\n",
    "\n",
    "        :param a: Valor actual.\n",
    "        :param g: Gradiente.\n",
    "        :param lr: Tasa de aprendizaje.\n",
    "        :return: Valor actualizado.\n",
    "        \"\"\"\n",
    "        return a - (g * lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
